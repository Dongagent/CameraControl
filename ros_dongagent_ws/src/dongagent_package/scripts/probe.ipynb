{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0f2c42-5240-4843-8f9b-160a7021a428",
   "metadata": {},
   "source": [
    "# show the max and min in each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2e22af3-2ad7-493f-bb95-cf69fdc228cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "max: 0.8908626\n",
      "min: 0.00022180197\n",
      "disgust\n",
      "max: 0.9448169\n",
      "min: 2.9978316e-06\n",
      "fear\n",
      "max: 0.7755312\n",
      "min: 0.00032007878\n",
      "happiness\n",
      "max: 0.88944674\n",
      "min: 0.00030392586\n",
      "sadness\n",
      "max: 0.92630714\n",
      "min: 0.0006400511\n",
      "surprise\n",
      "max: 0.9870663\n",
      "min: 0.00094844046\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "#     CURR_EMO = 'anger'\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "\n",
    "    for img in images:\n",
    "        test = os.path.join(myfolder, CURR_EMO, img)\n",
    "        csvname = test.split('.png')[0] + '_emotion.csv'\n",
    "        tmp = pd.read_csv(csvname)\n",
    "        result[CURR_EMO].append(float(tmp[CURR_EMO]))\n",
    "\n",
    "    print(CURR_EMO)\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0513-4c05-47f6-be22-3752bbf33d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for each emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ac4e015-90e5-49b2-9a6f-2915dbc6752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': [0.56476116, 0.45512825, 0.0053509707, 0.012822615, 0.55845755],\n",
       " 'disgust': [6.644751e-06, 0.05863482, 0.78763676, 0.0036472667, 0.8512238],\n",
       " 'fear': [0.51877135, 0.03438015, 0.0143282255, 0.016819654, 0.00056438893],\n",
       " 'happiness': [0.05496557, 0.010495846, 0.64524376, 0.71323854, 0.10639376],\n",
       " 'sadness': [0.671066, 0.014642206, 0.80606645, 0.76308423, 0.7435],\n",
       " 'surprise': [0.9768141, 0.8902508, 0.9434955, 0.8905662, 0.843911]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8708bf37-00d8-411e-b820-840bb058720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56476116"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(tmp[CURR_EMO])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ce39d-d87a-410a-9c65-b90c574ccdba",
   "metadata": {},
   "source": [
    "# Model range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a174f12-6be8-4628-a3cc-d6543b21cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from intensityNet import * \n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "def intensityNet_analysis(img, target_emotion, is_save_csv=True):\n",
    "    # remember to set it before doing analysis\n",
    "\n",
    "    global intensityModel\n",
    "    # use intensityModel to detect emo\n",
    "    detection_res = intensityModel.detect_emo(Image.open(img))\n",
    "    detection_res = detection_res.tolist()\n",
    "    output = detection_res[get_target(target_emotion)]\n",
    "\n",
    "#     # create a pd dataframe\n",
    "#     detection_res = pd.DataFrame([detection_res], columns=[\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"])\n",
    "#     # Create DataFrame\n",
    "\n",
    "#     if is_save_csv:\n",
    "#         csv_emotion_name = img[:-4]+\"_intensitynet.csv\"\n",
    "#         detection_res.to_csv(csv_emotion_name)\n",
    "\n",
    "    # result = tmp_res[target_emotion]\n",
    "    return output\n",
    "\n",
    "def setIntensityModel(target_emotion):\n",
    "    global intensityModel\n",
    "    # Load the model\n",
    "    # ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "    print(\"target_emotion:\", target_emotion)\n",
    "    if target_emotion.lower() in [\"anger\", 'angry']:\n",
    "        model_path = \"new_models/angry_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"disgust\"]:\n",
    "        model_path = \"new_models/disgust_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"fear\"]:\n",
    "        model_path = \"new_models/fear_fold3_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"happiness\", \"happy\"]:\n",
    "        model_path = \"new_models/happy_fold2_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"sadness\", \"sad\"]:\n",
    "        model_path = \"new_models/sad_fold2_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"surprise\"]:\n",
    "        model_path = \"new_models/surprise_fold3_epoch5.pt\"\n",
    "    else:\n",
    "        model_path = \"\"\n",
    "\n",
    "    intensityModel = IntensityNet_type1(model_path)\n",
    "    return 1\n",
    "\n",
    "# intensityNet_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df3624bd-cb4a-4424-9ce2-f56e5305c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n",
      "anger\n",
      "500\n",
      "max: 0.6442480683326721\n",
      "min: 0.390322208404541\n",
      "target_emotion: disgust\n",
      "disgust\n",
      "500\n",
      "max: 0.6250965595245361\n",
      "min: 0.35516366362571716\n",
      "target_emotion: fear\n",
      "fear\n",
      "500\n",
      "max: 0.4678744375705719\n",
      "min: 0.19767922163009644\n",
      "target_emotion: happiness\n",
      "happiness\n",
      "500\n",
      "max: 0.5652247071266174\n",
      "min: 0.2887384295463562\n",
      "target_emotion: sadness\n",
      "sadness\n",
      "500\n",
      "max: 0.6272915601730347\n",
      "min: 0.33632296323776245\n",
      "target_emotion: surprise\n",
      "surprise\n",
      "500\n",
      "max: 0.45335686206817627\n",
      "min: 0.23851583898067474\n"
     ]
    }
   ],
   "source": [
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "\n",
    "# CURR_EMO = 'anger'\n",
    "\n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "    setIntensityModel(CURR_EMO)\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "    \n",
    "    for i in images:\n",
    "        result[CURR_EMO].append(intensityNet_analysis(os.path.join(myfolder, CURR_EMO, images[0]), CURR_EMO))\n",
    "        \n",
    "    print(CURR_EMO)\n",
    "    print(len(result[CURR_EMO]))\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94237e0b-b98e-4bc5-9538-90e35b203f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the mixed model\n",
    "\n",
    "# Function to calculate the mixed output with nonlinear transition using a sigmoid function\n",
    "def calculate_output_nonlinear(A, B, threshold=0.75, alpha=0.8, B_min=0.39, B_max=0.64, output_min=0.75, output_max=1.2, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mixed output based on values of A and B, with a threshold-based decision rule.\n",
    "    \n",
    "    Parameters:\n",
    "    A (float): Value from distribution A.\n",
    "    B (float): Value from distribution B.\n",
    "    threshold (float): Threshold for A to determine when to mix. Default is 0.75.\n",
    "    alpha (float): Weighting factor for B' in the mix when A > threshold. Default is 0.8.\n",
    "    B_min (float): Minimum value of B's original range. Default is 0.39.\n",
    "    B_max (float): Maximum value of B's original range. Default is 0.64.\n",
    "    output_min (float): Minimum value of the target range for B' (after scaling). Default is 0.75.\n",
    "    output_max (float): Maximum value of the target range for B' (after scaling). Default is 1.2.\n",
    "\n",
    "    Returns:\n",
    "    float: Final output value based on A and B.\n",
    "    \"\"\"\n",
    "        \n",
    "    # If A is below or equal to the threshold, output A directly\n",
    "    if A <= threshold:\n",
    "        return A\n",
    "    \n",
    "    # Scale B to fit within the desired range [output_min, output_max]\n",
    "    B_mapped = output_min + (B - B_min) * (output_max - output_min) / (B_max - B_min)\n",
    "    \n",
    "    # Apply a sigmoid-based weight for smooth transition\n",
    "    weight = 1 / (1 + np.exp(-k * (A - threshold)))  # Sigmoid function for smoother blending\n",
    "    \n",
    "    # Calculate the smooth nonlinear mixed output\n",
    "    output = weight * (alpha * B_mapped + (1 - alpha) * A) + (1 - weight) * A\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dad58e-cd47-4f37-9c04-6173c930e5d2",
   "metadata": {},
   "source": [
    "## Test the mixed model on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb527801-c23e-451a-960d-1c3b323c5761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dad05b0-2cb5-49da-8704-72b7853cae76",
   "metadata": {},
   "source": [
    "# extract probe from past experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5cdb-3389-45f0-bdd2-8c9b12e96f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def omega(x):\n",
    "    return 1.0 / (1 + np.exp(-k * ( x - threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dd81b-930c-4dca-a38a-fc400400e9d3",
   "metadata": {},
   "source": [
    "# difference between pyfeat value and resmasknet value (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388f01d-a9bc-451d-9fdf-1b77009565ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyfeat value\n",
    "from feat import Detector\n",
    "import os\n",
    "import cv2\n",
    "from SiameseRankNet import *\n",
    "from resmasknet import *\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "detector = Detector(emotion_model = \"resmasknet\", landmark_model='mobilefacenet')\n",
    "\n",
    "def get_face_box(start_x, start_y, end_x, end_y):\n",
    "    center_x, center_y = (start_x + end_x) // 2, (start_y + end_y) // 2\n",
    "    square_length = ((end_x - start_x) + (end_y - start_y)) // 2 // 2\n",
    "    square_length *= 1.1\n",
    "    start_x = int(center_x - square_length)\n",
    "    start_y = int(center_y - square_length)\n",
    "    end_x = int(center_x + square_length)\n",
    "    end_y = int(center_y + square_length)\n",
    "    return start_x, start_y, end_x, end_y\n",
    "\n",
    "\n",
    "result = {}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "CURR_EMO = 'anger'\n",
    "images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "image_size = (224, 224)\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.ToTensor()]\n",
    "        )\n",
    "model = ResMaskNet()\n",
    "for i in images[:1]:\n",
    "    print(i)\n",
    "    test = os.path.join(myfolder, CURR_EMO, i)\n",
    "    res = detector.detect_image(test)\n",
    "    facebox = res.iloc[:,1:5]\n",
    "    emotions = res.iloc[:, -8:]\n",
    "    print(f'use pyfeat:\\n{[round(x, 5) for x in np.array(emotions.iloc[:, :]).tolist()[0][:-1]]}')\n",
    "    frame = cv2.imread(test)\n",
    "    face = detector.detect_faces(frame)\n",
    "    emotion = model.detect_emo(frame=frame, detected_face=face)\n",
    "    print(f'use model directly:\\n{list(round(x, 5) for x in emotion[0].tolist())}')\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37pyfeat1",
   "language": "python",
   "name": "py37pyfeat1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
