{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0f2c42-5240-4843-8f9b-160a7021a428",
   "metadata": {},
   "source": [
    "# show the max and min in each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2e22af3-2ad7-493f-bb95-cf69fdc228cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "max: 0.8908626\n",
      "min: 0.00022180197\n",
      "disgust\n",
      "max: 0.9448169\n",
      "min: 2.9978316e-06\n",
      "fear\n",
      "max: 0.7755312\n",
      "min: 0.00032007878\n",
      "happiness\n",
      "max: 0.88944674\n",
      "min: 0.00030392586\n",
      "sadness\n",
      "max: 0.92630714\n",
      "min: 0.0006400511\n",
      "surprise\n",
      "max: 0.9870663\n",
      "min: 0.00094844046\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "#     CURR_EMO = 'anger'\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "\n",
    "    for img in images:\n",
    "        test = os.path.join(myfolder, CURR_EMO, img)\n",
    "        csvname = test.split('.png')[0] + '_emotion.csv'\n",
    "        tmp = pd.read_csv(csvname)\n",
    "        result[CURR_EMO].append(float(tmp[CURR_EMO]))\n",
    "\n",
    "    print(CURR_EMO)\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0513-4c05-47f6-be22-3752bbf33d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for each emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ac4e015-90e5-49b2-9a6f-2915dbc6752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': [0.56476116, 0.45512825, 0.0053509707, 0.012822615, 0.55845755],\n",
       " 'disgust': [6.644751e-06, 0.05863482, 0.78763676, 0.0036472667, 0.8512238],\n",
       " 'fear': [0.51877135, 0.03438015, 0.0143282255, 0.016819654, 0.00056438893],\n",
       " 'happiness': [0.05496557, 0.010495846, 0.64524376, 0.71323854, 0.10639376],\n",
       " 'sadness': [0.671066, 0.014642206, 0.80606645, 0.76308423, 0.7435],\n",
       " 'surprise': [0.9768141, 0.8902508, 0.9434955, 0.8905662, 0.843911]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8708bf37-00d8-411e-b820-840bb058720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56476116"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(tmp[CURR_EMO])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ce39d-d87a-410a-9c65-b90c574ccdba",
   "metadata": {},
   "source": [
    "# Model range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a174f12-6be8-4628-a3cc-d6543b21cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from intensityNet import * \n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "def intensityNet_analysis(img, target_emotion, is_save_csv=True):\n",
    "    # remember to set it before doing analysis\n",
    "\n",
    "    global intensityModel\n",
    "    # use intensityModel to detect emo\n",
    "    detection_res = intensityModel.detect_emo(Image.open(img))\n",
    "    detection_res = detection_res.tolist()\n",
    "    output = detection_res[get_target(target_emotion)]\n",
    "\n",
    "#     # create a pd dataframe\n",
    "#     detection_res = pd.DataFrame([detection_res], columns=[\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"])\n",
    "#     # Create DataFrame\n",
    "\n",
    "#     if is_save_csv:\n",
    "#         csv_emotion_name = img[:-4]+\"_intensitynet.csv\"\n",
    "#         detection_res.to_csv(csv_emotion_name)\n",
    "\n",
    "    # result = tmp_res[target_emotion]\n",
    "    return output\n",
    "\n",
    "def setIntensityModel(target_emotion):\n",
    "    global intensityModel\n",
    "    # Load the model\n",
    "    # ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "    print(\"target_emotion:\", target_emotion)\n",
    "    if target_emotion.lower() in [\"anger\", 'angry']:\n",
    "        model_path = \"new_models/angry_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"disgust\"]:\n",
    "        model_path = \"new_models/disgust_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"fear\"]:\n",
    "        model_path = \"new_models/fear_fold3_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"happiness\", \"happy\"]:\n",
    "        model_path = \"new_models/happy_fold2_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"sadness\", \"sad\"]:\n",
    "        model_path = \"new_models/sad_fold2_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"surprise\"]:\n",
    "        model_path = \"new_models/surprise_fold3_epoch5.pt\"\n",
    "    else:\n",
    "        model_path = \"\"\n",
    "\n",
    "    intensityModel = IntensityNet_type1(model_path)\n",
    "    return 1\n",
    "\n",
    "# intensityNet_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df3624bd-cb4a-4424-9ce2-f56e5305c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n",
      "anger\n",
      "500\n",
      "max: 0.6442480683326721\n",
      "min: 0.390322208404541\n",
      "target_emotion: disgust\n",
      "disgust\n",
      "500\n",
      "max: 0.6250965595245361\n",
      "min: 0.35516366362571716\n",
      "target_emotion: fear\n",
      "fear\n",
      "500\n",
      "max: 0.4678744375705719\n",
      "min: 0.19767922163009644\n",
      "target_emotion: happiness\n",
      "happiness\n",
      "500\n",
      "max: 0.5652247071266174\n",
      "min: 0.2887384295463562\n",
      "target_emotion: sadness\n",
      "sadness\n",
      "500\n",
      "max: 0.6272915601730347\n",
      "min: 0.33632296323776245\n",
      "target_emotion: surprise\n",
      "surprise\n",
      "500\n",
      "max: 0.45335686206817627\n",
      "min: 0.23851583898067474\n"
     ]
    }
   ],
   "source": [
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "\n",
    "# CURR_EMO = 'anger'\n",
    "\n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "    setIntensityModel(CURR_EMO)\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "    \n",
    "    for i in images:\n",
    "        result[CURR_EMO].append(intensityNet_analysis(os.path.join(myfolder, CURR_EMO, images[0]), CURR_EMO))\n",
    "        \n",
    "    print(CURR_EMO)\n",
    "    print(len(result[CURR_EMO]))\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94237e0b-b98e-4bc5-9538-90e35b203f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the mixed model\n",
    "\n",
    "# Function to calculate the mixed output with nonlinear transition using a sigmoid function\n",
    "def calculate_output_nonlinear(A, B, threshold=0.75, alpha=0.8, B_min=0.39, B_max=0.64, output_min=0.75, output_max=1.2, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mixed output based on values of A and B, with a threshold-based decision rule.\n",
    "    \n",
    "    Parameters:\n",
    "    A (float): Value from distribution A.\n",
    "    B (float): Value from distribution B.\n",
    "    threshold (float): Threshold for A to determine when to mix. Default is 0.75.\n",
    "    alpha (float): Weighting factor for B' in the mix when A > threshold. Default is 0.8.\n",
    "    B_min (float): Minimum value of B's original range. Default is 0.39.\n",
    "    B_max (float): Maximum value of B's original range. Default is 0.64.\n",
    "    output_min (float): Minimum value of the target range for B' (after scaling). Default is 0.75.\n",
    "    output_max (float): Maximum value of the target range for B' (after scaling). Default is 1.2.\n",
    "\n",
    "    Returns:\n",
    "    float: Final output value based on A and B.\n",
    "    \"\"\"\n",
    "        \n",
    "    # If A is below or equal to the threshold, output A directly\n",
    "    if A <= threshold:\n",
    "        return A\n",
    "    \n",
    "    # Scale B to fit within the desired range [output_min, output_max]\n",
    "    B_mapped = output_min + (B - B_min) * (output_max - output_min) / (B_max - B_min)\n",
    "    \n",
    "    # Apply a sigmoid-based weight for smooth transition\n",
    "    weight = 1 / (1 + np.exp(-k * (A - threshold)))  # Sigmoid function for smoother blending\n",
    "    \n",
    "    # Calculate the smooth nonlinear mixed output\n",
    "    output = weight * (alpha * B_mapped + (1 - alpha) * A) + (1 - weight) * A\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dad58e-cd47-4f37-9c04-6173c930e5d2",
   "metadata": {},
   "source": [
    "## Test the mixed model on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb527801-c23e-451a-960d-1c3b323c5761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dad05b0-2cb5-49da-8704-72b7853cae76",
   "metadata": {},
   "source": [
    "# extract probe from past experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5cdb-3389-45f0-bdd2-8c9b12e96f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def omega(x):\n",
    "    return 1.0 / (1 + np.exp(-k * ( x - threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dd81b-930c-4dca-a38a-fc400400e9d3",
   "metadata": {},
   "source": [
    "# difference between pyfeat value and resmasknet value (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e388f01d-a9bc-451d-9fdf-1b77009565ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detection model:  retinaface\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilefacenet_model_best.pth.tar\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/RF_568.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Loading Face Landmark model:  mobilefacenet\n",
      "Loading au model:  rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion model:  resmasknet\n",
      "2023_02_10_15_13_31_anger_49.png\n",
      "use pyfeat:\n",
      "[0.67939, 0.0903, 0.06065, 0.02487, 0.05022, 0.06415, 0.03042]\n",
      "[array([0.67939264, 0.09030051, 0.06065326, 0.02487272, 0.05021691,\n",
      "       0.06414597, 0.03041801], dtype=float32)]\n",
      "use model directly:\n",
      "[0.67939, 0.0903, 0.06065, 0.02487, 0.05022, 0.06415, 0.03042]\n",
      "use pyfeat again:\n",
      "[0.67939, 0.0903, 0.06065, 0.02487, 0.05022, 0.06415, 0.03042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyfeat value\n",
    "from feat import Detector\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "# from SiameseRankNet import *\n",
    "from resmasknet import ResMaskNet\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "detector = Detector(emotion_model = \"resmasknet\", landmark_model='mobilefacenet')\n",
    "\n",
    "def get_face_box(start_x, start_y, end_x, end_y):\n",
    "    center_x, center_y = (start_x + end_x) // 2, (start_y + end_y) // 2\n",
    "    square_length = ((end_x - start_x) + (end_y - start_y)) // 2 // 2\n",
    "    square_length *= 1.1\n",
    "    start_x = int(center_x - square_length)\n",
    "    start_y = int(center_y - square_length)\n",
    "    end_x = int(center_x + square_length)\n",
    "    end_y = int(center_y + square_length)\n",
    "    return start_x, start_y, end_x, end_y\n",
    "\n",
    "\n",
    "result = {}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "CURR_EMO = 'anger'\n",
    "images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "image_size = (224, 224)\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.ToTensor()]\n",
    "        )\n",
    "model = ResMaskNet()\n",
    "for i in images[:1]:\n",
    "    print(i)\n",
    "    test = os.path.join(myfolder, CURR_EMO, i)\n",
    "    res = detector.detect_image(test)\n",
    "    facebox = res.iloc[:,1:5]\n",
    "    emotions = res.iloc[:, -8:]\n",
    "    print(f'use pyfeat:\\n{[round(x, 5) for x in np.array(emotions.iloc[:, :]).tolist()[0][:-1]]}')\n",
    "    \n",
    "    frame = cv2.imread(test)\n",
    "    face = detector.detect_faces(frame)\n",
    "    emotion = model.detect_emo(frame=frame, detected_face=face)\n",
    "    print(emotion)\n",
    "    print(f'use model directly:\\n{list(round(x, 5) for x in emotion[0].tolist())}')\n",
    "    \n",
    "    test = os.path.join(myfolder, CURR_EMO, i)\n",
    "    res = detector.detect_image(test)\n",
    "    facebox = res.iloc[:,1:5]\n",
    "    emotions = res.iloc[:, -8:]\n",
    "    print(f'use pyfeat again:\\n{[round(x, 5) for x in np.array(emotions.iloc[:, :]).tolist()[0][:-1]]}')\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab6b39f-c927-488d-b313-0bb7063b9b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[226.40732, 110.24718, 415.84763, 378.30368, 0.99957854]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c1ad93-37a8-4ef5-a294-cd1e3c76d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      anger   disgust      fear  happiness   sadness  surprise   neutral\n",
      "0  0.679393  0.090301  0.060653   0.024873  0.050217  0.064146  0.030418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.DataFrame(emotion, columns=[\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\"])\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd5c61-6f7a-4d6b-b83c-f093cc9ef1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ebdcc7-ba23-41fb-a9f7-86457be9c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142900/3267738695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtarget_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'disgust'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfacebox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mrmn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResMaskNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtargetID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detector' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feat import Detector\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def get_target(emotion_name):\n",
    "    # Anger, Disgust, Fear, Happiness, Sadness, Surprise, Neutral\n",
    "    # or lowercase\n",
    "    if emotion_name in [\"Anger\", \"anger\"]:\n",
    "        return 0\n",
    "    elif emotion_name in [\"Disgust\", \"disgust\"]:\n",
    "        return 1\n",
    "    elif emotion_name in [\"Fear\", \"fear\"]:\n",
    "        return 2\n",
    "    elif emotion_name in [\"Happiness\", \"happiness\"]:\n",
    "        return 3\n",
    "    elif emotion_name in [\"Sadness\", \"sadness\"]:\n",
    "        return 4\n",
    "    elif emotion_name in [\"Surprise\", \"surprise\"]:\n",
    "        return 5\n",
    "    elif emotion_name in [\"Neutral\", \"neutral\"]:\n",
    "        return 6\n",
    "image_name = 'image_analysis/fear/2024_11_14_17_14_20_fear_0.png'\n",
    "target_emotion = 'disgust'\n",
    "\n",
    "facebox = detector.detect_faces(cv2.imread(image_name))[0]\n",
    "rmn_model = ResMaskNet()\n",
    "targetID = get_target(target_emotion)\n",
    "print(image_name)\n",
    "rmn_res = rmn_model.detect_emo(frame=cv2.imread(image_name), detected_face=[facebox])\n",
    "new_df = pd.DataFrame(rmn_res, columns=[\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\"])\n",
    "new_df['input'] = image_name\n",
    "\n",
    "\n",
    "# -----------\n",
    "# old version\n",
    "# -----------\n",
    "image_prediction = detector.detect_image(image_name)\n",
    "df = image_prediction.head()\n",
    "\n",
    "targetID = get_target(target_emotion)\n",
    "\n",
    "assert list(new_df[target_emotion])[0] == list(df[target_emotion])[0], \"The result is not the same, old: {}, new: {}\".format(list(df[target_emotion])[0], list(new_df[target_emotion])[0])\n",
    "# return emo_df.iloc[0,targetID]\n",
    "print(new_df[target_emotion])\n",
    "print(df[target_emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac933d1-46c4-472f-97ce-eabbdf1c9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import feat\n",
    "print(feat.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d3f92c-ed84-43b7-b962-cd2da9d6197a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Detector' has no attribute '__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142900/3539268366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Detector' has no attribute '__file__'"
     ]
    }
   ],
   "source": [
    "from feat import Detector\n",
    "print(Detector.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959335b0-d0a0-47f1-8a97-b7bf57c91f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detection model:  retinaface\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilefacenet_model_best.pth.tar\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/RF_568.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Loading Face Landmark model:  mobilefacenet\n",
      "Loading au model:  rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion model:  resmasknet\n"
     ]
    }
   ],
   "source": [
    "from feat import Detector\n",
    "\n",
    "detector = Detector(emotion_model = \"resmasknet\", landmark_model='mobilefacenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc8143d-788c-4e73-a748-a3fb2175fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(detector.detect_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7092830-5274-43ef-b49f-e03ea43e6765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': (0, 255), 'x6': (0, 255), 'x8': (-255, 255), 'x10': (0, 255), 'x11': (0, 255), 'x16': (0, 255), 'x18': (-255, 255), 'x20': (0, 255), 'x28': (0, 255), 'x29': (0, 255), 'x30': (0, 255), 'x32': (0, 110)}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def generate_pbounds(axes, target_emotion):\n",
    "        pbounds_dic = {}\n",
    "        for i in axes:\n",
    "            if target_emotion == 'happiness' and i in [0, 1]:\n",
    "                # TODO need to verify\n",
    "                happy_upper = 120\n",
    "                pbounds_dic['x{}'.format(i)] = (0, happy_upper)\n",
    "            # elif i in [0, 1]:\n",
    "            #     pbounds_dic['x{}'.format(i)] = (0, 160)\n",
    "            # else:\n",
    "            pbounds_dic['x{}'.format(i)] = (0, 255)\n",
    "\n",
    "        # mouth\n",
    "        pbounds_dic['x32'] = (0, 110)\n",
    "\n",
    "        # dangerous point\n",
    "        pbounds_dic['x8'] = (-255, 255)\n",
    "        pbounds_dic['x18'] = (-255, 255)\n",
    "\n",
    "        print(pbounds_dic)\n",
    "        return pbounds_dic\n",
    "    \n",
    "target_emotion = 'target_emotion'\n",
    "all_axes_for_emotions = [1, 6, 8, 10, 11, 16, 18, 20, 28, 29, 30, 32]\n",
    "pbounds = generate_pbounds(all_axes_for_emotions, target_emotion)\n",
    "\n",
    "def middle_function(**kwargs):\n",
    "    print(kwargs)\n",
    "    return 'haha'\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "        f=middle_function,\n",
    "        # Define HyperParameter Space\n",
    "        # acquisition_function=acq,\n",
    "        pbounds = pbounds,\n",
    "        random_state=486,\n",
    "        verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ae305b-44b3-43b7-8032-5916a083ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/bayes_opt/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import bayes_opt\n",
    "print(bayes_opt.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823f9dca-a068-40ec-a337-dffa9f6e5e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 141.4699283283842, 'x10': 1.6114745652063145, 'x11': 125.33073461509736, 'x16': 161.21965120867395, 'x18': -231.7199772728022, 'x20': 184.38544953604224, 'x28': 105.81132684074315, 'x29': 186.3368351761186, 'x30': 55.62169212632774, 'x32': 58.840452241202755, 'x6': 112.88227631185735, 'x8': 74.65535775835122}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "ucb = UtilityFunction(kind='ucb',\n",
    "                          kappa=2.576,\n",
    "                          xi=0.0,\n",
    "                          kappa_decay=1,\n",
    "                          kappa_decay_delay=0)\n",
    "print(optimizer.suggest(ucb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37pyfeat1",
   "language": "python",
   "name": "py37pyfeat1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
