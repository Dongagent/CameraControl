{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0f2c42-5240-4843-8f9b-160a7021a428",
   "metadata": {},
   "source": [
    "# show the max and min in each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2e22af3-2ad7-493f-bb95-cf69fdc228cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger\n",
      "max: 0.8908626\n",
      "min: 0.00022180197\n",
      "disgust\n",
      "max: 0.9448169\n",
      "min: 2.9978316e-06\n",
      "fear\n",
      "max: 0.7755312\n",
      "min: 0.00032007878\n",
      "happiness\n",
      "max: 0.88944674\n",
      "min: 0.00030392586\n",
      "sadness\n",
      "max: 0.92630714\n",
      "min: 0.0006400511\n",
      "surprise\n",
      "max: 0.9870663\n",
      "min: 0.00094844046\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "#     CURR_EMO = 'anger'\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "\n",
    "    for img in images:\n",
    "        test = os.path.join(myfolder, CURR_EMO, img)\n",
    "        csvname = test.split('.png')[0] + '_emotion.csv'\n",
    "        tmp = pd.read_csv(csvname)\n",
    "        result[CURR_EMO].append(float(tmp[CURR_EMO]))\n",
    "\n",
    "    print(CURR_EMO)\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc0513-4c05-47f6-be22-3752bbf33d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for each emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ac4e015-90e5-49b2-9a6f-2915dbc6752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': [0.56476116, 0.45512825, 0.0053509707, 0.012822615, 0.55845755],\n",
       " 'disgust': [6.644751e-06, 0.05863482, 0.78763676, 0.0036472667, 0.8512238],\n",
       " 'fear': [0.51877135, 0.03438015, 0.0143282255, 0.016819654, 0.00056438893],\n",
       " 'happiness': [0.05496557, 0.010495846, 0.64524376, 0.71323854, 0.10639376],\n",
       " 'sadness': [0.671066, 0.014642206, 0.80606645, 0.76308423, 0.7435],\n",
       " 'surprise': [0.9768141, 0.8902508, 0.9434955, 0.8905662, 0.843911]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8708bf37-00d8-411e-b820-840bb058720f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56476116"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(tmp[CURR_EMO])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ce39d-d87a-410a-9c65-b90c574ccdba",
   "metadata": {},
   "source": [
    "# Model range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a174f12-6be8-4628-a3cc-d6543b21cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from intensityNet import * \n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "def intensityNet_analysis(img, target_emotion, is_save_csv=True):\n",
    "    # remember to set it before doing analysis\n",
    "\n",
    "    global intensityModel\n",
    "    # use intensityModel to detect emo\n",
    "    detection_res = intensityModel.detect_emo(Image.open(img))\n",
    "    detection_res = detection_res.tolist()\n",
    "    output = detection_res[get_target(target_emotion)]\n",
    "\n",
    "#     # create a pd dataframe\n",
    "#     detection_res = pd.DataFrame([detection_res], columns=[\"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"])\n",
    "#     # Create DataFrame\n",
    "\n",
    "#     if is_save_csv:\n",
    "#         csv_emotion_name = img[:-4]+\"_intensitynet.csv\"\n",
    "#         detection_res.to_csv(csv_emotion_name)\n",
    "\n",
    "    # result = tmp_res[target_emotion]\n",
    "    return output\n",
    "\n",
    "def setIntensityModel(target_emotion):\n",
    "    global intensityModel\n",
    "    # Load the model\n",
    "    # ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "    print(\"target_emotion:\", target_emotion)\n",
    "    if target_emotion.lower() in [\"anger\", 'angry']:\n",
    "        model_path = \"new_models/angry_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"disgust\"]:\n",
    "        model_path = \"new_models/disgust_fold3_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"fear\"]:\n",
    "        model_path = \"new_models/fear_fold3_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"happiness\", \"happy\"]:\n",
    "        model_path = \"new_models/happy_fold2_epoch7.pt\"\n",
    "    elif target_emotion.lower() in [\"sadness\", \"sad\"]:\n",
    "        model_path = \"new_models/sad_fold2_epoch6.pt\"\n",
    "    elif target_emotion.lower() in [\"surprise\"]:\n",
    "        model_path = \"new_models/surprise_fold3_epoch5.pt\"\n",
    "    else:\n",
    "        model_path = \"\"\n",
    "\n",
    "    intensityModel = IntensityNet_type1(model_path)\n",
    "    return 1\n",
    "\n",
    "# intensityNet_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df3624bd-cb4a-4424-9ce2-f56e5305c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_emotion: anger\n",
      "anger\n",
      "500\n",
      "max: 0.6442480683326721\n",
      "min: 0.390322208404541\n",
      "target_emotion: disgust\n",
      "disgust\n",
      "500\n",
      "max: 0.6250965595245361\n",
      "min: 0.35516366362571716\n",
      "target_emotion: fear\n",
      "fear\n",
      "500\n",
      "max: 0.4678744375705719\n",
      "min: 0.19767922163009644\n",
      "target_emotion: happiness\n",
      "happiness\n",
      "500\n",
      "max: 0.5652247071266174\n",
      "min: 0.2887384295463562\n",
      "target_emotion: sadness\n",
      "sadness\n",
      "500\n",
      "max: 0.6272915601730347\n",
      "min: 0.33632296323776245\n",
      "target_emotion: surprise\n",
      "surprise\n",
      "500\n",
      "max: 0.45335686206817627\n",
      "min: 0.23851583898067474\n"
     ]
    }
   ],
   "source": [
    "emolist = 'anger, disgust, fear, happiness, sadness, surprise'.split(', ')\n",
    "result = {x:[] for x in emolist}\n",
    "myfolder = 'image_analysis/Exp_collections/230210Exp13Lighting_sys_500/'\n",
    "\n",
    "\n",
    "# CURR_EMO = 'anger'\n",
    "\n",
    "global intensityModel\n",
    "intensityModel = ''\n",
    "\n",
    "\n",
    "\n",
    "for CURR_EMO in emolist:\n",
    "    setIntensityModel(CURR_EMO)\n",
    "    images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "    \n",
    "    for i in images:\n",
    "        result[CURR_EMO].append(intensityNet_analysis(os.path.join(myfolder, CURR_EMO, images[0]), CURR_EMO))\n",
    "        \n",
    "    print(CURR_EMO)\n",
    "    print(len(result[CURR_EMO]))\n",
    "    print(f'max: {max(result[CURR_EMO])}')\n",
    "    print(f'min: {min(result[CURR_EMO])}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94237e0b-b98e-4bc5-9538-90e35b203f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the mixed model\n",
    "\n",
    "# Function to calculate the mixed output with nonlinear transition using a sigmoid function\n",
    "def calculate_output_nonlinear(A, B, threshold=0.75, alpha=0.8, B_min=0.39, B_max=0.64, output_min=0.75, output_max=1.2, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the mixed output based on values of A and B, with a threshold-based decision rule.\n",
    "    \n",
    "    Parameters:\n",
    "    A (float): Value from distribution A.\n",
    "    B (float): Value from distribution B.\n",
    "    threshold (float): Threshold for A to determine when to mix. Default is 0.75.\n",
    "    alpha (float): Weighting factor for B' in the mix when A > threshold. Default is 0.8.\n",
    "    B_min (float): Minimum value of B's original range. Default is 0.39.\n",
    "    B_max (float): Maximum value of B's original range. Default is 0.64.\n",
    "    output_min (float): Minimum value of the target range for B' (after scaling). Default is 0.75.\n",
    "    output_max (float): Maximum value of the target range for B' (after scaling). Default is 1.2.\n",
    "\n",
    "    Returns:\n",
    "    float: Final output value based on A and B.\n",
    "    \"\"\"\n",
    "        \n",
    "    # If A is below or equal to the threshold, output A directly\n",
    "    if A <= threshold:\n",
    "        return A\n",
    "    \n",
    "    # Scale B to fit within the desired range [output_min, output_max]\n",
    "    B_mapped = output_min + (B - B_min) * (output_max - output_min) / (B_max - B_min)\n",
    "    \n",
    "    # Apply a sigmoid-based weight for smooth transition\n",
    "    weight = 1 / (1 + np.exp(-k * (A - threshold)))  # Sigmoid function for smoother blending\n",
    "    \n",
    "    # Calculate the smooth nonlinear mixed output\n",
    "    output = weight * (alpha * B_mapped + (1 - alpha) * A) + (1 - weight) * A\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dad58e-cd47-4f37-9c04-6173c930e5d2",
   "metadata": {},
   "source": [
    "## Test the mixed model on old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb527801-c23e-451a-960d-1c3b323c5761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dad05b0-2cb5-49da-8704-72b7853cae76",
   "metadata": {},
   "source": [
    "# extract probe from past experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5cdb-3389-45f0-bdd2-8c9b12e96f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def omega(x):\n",
    "    return 1.0 / (1 + np.exp(-k * ( x - threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dd81b-930c-4dca-a38a-fc400400e9d3",
   "metadata": {},
   "source": [
    "# difference between pyfeat value and resmasknet value (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e388f01d-a9bc-451d-9fdf-1b77009565ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detection model:  retinaface\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilefacenet_model_best.pth.tar\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/RF_568.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Loading Face Landmark model:  mobilefacenet\n",
      "Loading au model:  rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion model:  resmasknet\n",
      "2024_11_17_18_11_17_anger_104.png\n",
      "use pyfeat:\n",
      "[0.00641, 0.00552, 0.04838, 0.63147, 0.02117, 0.10476, 0.18228]\n",
      "[array([0.00641471, 0.00551518, 0.04838388, 0.6314743 , 0.02117071,\n",
      "       0.10475855, 0.1822826 ], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00641, 0.00552, 0.04838, 0.63147, 0.02117, 0.10476, 0.18228]\n",
      "use pyfeat again:\n",
      "[0.00641, 0.00552, 0.04838, 0.63147, 0.02117, 0.10476, 0.18228]\n",
      "\n",
      "2024_11_17_18_10_11_anger_83.png\n",
      "use pyfeat:\n",
      "[0.02994, 0.06459, 0.19819, 0.31658, 0.13236, 0.02907, 0.22928]\n",
      "[array([0.02993941, 0.06458788, 0.19818795, 0.3165815 , 0.13235892,\n",
      "       0.029069  , 0.22927533], dtype=float32)]\n",
      "use model directly:\n",
      "[0.02994, 0.06459, 0.19819, 0.31658, 0.13236, 0.02907, 0.22928]\n",
      "use pyfeat again:\n",
      "[0.02994, 0.06459, 0.19819, 0.31658, 0.13236, 0.02907, 0.22928]\n",
      "\n",
      "2024_11_17_18_15_55_anger_189.png\n",
      "use pyfeat:\n",
      "[0.00648, 0.00494, 0.03865, 0.43532, 0.01181, 0.34213, 0.16067]\n",
      "[array([0.00648422, 0.00493549, 0.03864579, 0.4353175 , 0.01181063,\n",
      "       0.34213465, 0.1606717 ], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00648, 0.00494, 0.03865, 0.43532, 0.01181, 0.34213, 0.16067]\n",
      "use pyfeat again:\n",
      "[0.00648, 0.00494, 0.03865, 0.43532, 0.01181, 0.34213, 0.16067]\n",
      "\n",
      "2024_11_17_18_06_43_anger_16.png\n",
      "use pyfeat:\n",
      "[0.03787, 0.0084, 0.08626, 0.05449, 0.0678, 0.2192, 0.52598]\n",
      "[array([0.0378695 , 0.00839939, 0.08625643, 0.05448971, 0.06780346,\n",
      "       0.21919918, 0.52598226], dtype=float32)]\n",
      "use model directly:\n",
      "[0.03787, 0.0084, 0.08626, 0.05449, 0.0678, 0.2192, 0.52598]\n",
      "use pyfeat again:\n",
      "[0.03787, 0.0084, 0.08626, 0.05449, 0.0678, 0.2192, 0.52598]\n",
      "\n",
      "2024_11_17_18_09_37_anger_72.png\n",
      "use pyfeat:\n",
      "[0.00663, 0.00792, 0.02244, 0.37905, 0.01632, 0.15769, 0.40996]\n",
      "[array([0.00663233, 0.00791858, 0.0224411 , 0.37904552, 0.01631597,\n",
      "       0.15769047, 0.40995604], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00663, 0.00792, 0.02244, 0.37905, 0.01632, 0.15769, 0.40996]\n",
      "use pyfeat again:\n",
      "[0.00663, 0.00792, 0.02244, 0.37905, 0.01632, 0.15769, 0.40996]\n",
      "\n",
      "2024_11_17_18_14_58_anger_172.png\n",
      "use pyfeat:\n",
      "[0.00492, 0.00319, 0.03152, 0.45975, 0.00991, 0.37201, 0.11869]\n",
      "[array([0.00492166, 0.00318953, 0.03152284, 0.45975178, 0.00991453,\n",
      "       0.372014  , 0.11868577], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00492, 0.00319, 0.03152, 0.45975, 0.00991, 0.37201, 0.11869]\n",
      "use pyfeat again:\n",
      "[0.00492, 0.00319, 0.03152, 0.45975, 0.00991, 0.37201, 0.11869]\n",
      "\n",
      "2024_11_17_18_10_39_anger_92.png\n",
      "use pyfeat:\n",
      "[0.00541, 0.00466, 0.03867, 0.37473, 0.00604, 0.45748, 0.11301]\n",
      "[array([0.00540635, 0.00465882, 0.03866858, 0.3747349 , 0.0060378 ,\n",
      "       0.45748103, 0.1130125 ], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00541, 0.00466, 0.03867, 0.37473, 0.00604, 0.45748, 0.11301]\n",
      "use pyfeat again:\n",
      "[0.00541, 0.00466, 0.03867, 0.37473, 0.00604, 0.45748, 0.11301]\n",
      "\n",
      "2024_11_17_18_08_16_anger_46.png\n",
      "use pyfeat:\n",
      "[0.0052, 0.00448, 0.03515, 0.37702, 0.01973, 0.24455, 0.31387]\n",
      "[array([0.00520444, 0.004481  , 0.03514777, 0.37701556, 0.01972569,\n",
      "       0.24455076, 0.31387484], dtype=float32)]\n",
      "use model directly:\n",
      "[0.0052, 0.00448, 0.03515, 0.37702, 0.01973, 0.24455, 0.31387]\n",
      "use pyfeat again:\n",
      "[0.0052, 0.00448, 0.03515, 0.37702, 0.01973, 0.24455, 0.31387]\n",
      "\n",
      "2024_11_17_18_16_04_anger_192.png\n",
      "use pyfeat:\n",
      "[0.00247, 0.00193, 0.01765, 0.41471, 0.00741, 0.44493, 0.11091]\n",
      "[array([0.00246683, 0.00192684, 0.0176523 , 0.41470888, 0.00740961,\n",
      "       0.44492543, 0.11091011], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00247, 0.00193, 0.01765, 0.41471, 0.00741, 0.44493, 0.11091]\n",
      "use pyfeat again:\n",
      "[0.00247, 0.00193, 0.01765, 0.41471, 0.00741, 0.44493, 0.11091]\n",
      "\n",
      "2024_11_17_18_16_08_anger_193.png\n",
      "use pyfeat:\n",
      "[0.021, 0.04669, 0.10036, 0.34789, 0.05926, 0.02279, 0.40201]\n",
      "[array([0.021002  , 0.04669013, 0.10036229, 0.34788567, 0.05926012,\n",
      "       0.02278994, 0.40200993], dtype=float32)]\n",
      "use model directly:\n",
      "[0.021, 0.04669, 0.10036, 0.34789, 0.05926, 0.02279, 0.40201]\n",
      "use pyfeat again:\n",
      "[0.021, 0.04669, 0.10036, 0.34789, 0.05926, 0.02279, 0.40201]\n",
      "\n",
      "2024_11_17_18_08_22_anger_48.png\n",
      "use pyfeat:\n",
      "[0.01059, 0.01881, 0.15256, 0.30872, 0.0327, 0.07505, 0.40157]\n",
      "[array([0.01058833, 0.01881047, 0.1525581 , 0.30871964, 0.03269961,\n",
      "       0.07505137, 0.40157256], dtype=float32)]\n",
      "use model directly:\n",
      "[0.01059, 0.01881, 0.15256, 0.30872, 0.0327, 0.07505, 0.40157]\n",
      "use pyfeat again:\n",
      "[0.01059, 0.01881, 0.15256, 0.30872, 0.0327, 0.07505, 0.40157]\n",
      "\n",
      "2024_11_17_18_08_53_anger_58.png\n",
      "use pyfeat:\n",
      "[0.02087, 0.04679, 0.31076, 0.15335, 0.06841, 0.0415, 0.35832]\n",
      "[array([0.02087082, 0.04679193, 0.31076482, 0.15334716, 0.06840642,\n",
      "       0.04150192, 0.35831693], dtype=float32)]\n",
      "use model directly:\n",
      "[0.02087, 0.04679, 0.31076, 0.15335, 0.06841, 0.0415, 0.35832]\n",
      "use pyfeat again:\n",
      "[0.02087, 0.04679, 0.31076, 0.15335, 0.06841, 0.0415, 0.35832]\n",
      "\n",
      "2024_11_17_18_08_25_anger_49.png\n",
      "use pyfeat:\n",
      "[0.00935, 0.00342, 0.04204, 0.16184, 0.00753, 0.57748, 0.19834]\n",
      "[array([0.00935192, 0.00341701, 0.04204066, 0.16183846, 0.00753243,\n",
      "       0.57748044, 0.19833912], dtype=float32)]\n",
      "use model directly:\n",
      "[0.00935, 0.00342, 0.04204, 0.16184, 0.00753, 0.57748, 0.19834]\n",
      "use pyfeat again:\n",
      "[0.00935, 0.00342, 0.04204, 0.16184, 0.00753, 0.57748, 0.19834]\n",
      "\n",
      "2024_11_17_18_07_32_anger_32.png\n",
      "use pyfeat:\n",
      "[0.01163, 0.01315, 0.06287, 0.56938, 0.02023, 0.07499, 0.24776]\n",
      "[array([0.0116343 , 0.01314687, 0.06286631, 0.5693753 , 0.02023127,\n",
      "       0.07498805, 0.247758  ], dtype=float32)]\n",
      "use model directly:\n",
      "[0.01163, 0.01315, 0.06287, 0.56938, 0.02023, 0.07499, 0.24776]\n",
      "exception occurred\n",
      "use pyfeat again:\n",
      "[nan, nan, nan, nan, nan, nan, nan]\n",
      "\n",
      "2024_11_17_18_09_55_anger_78.png\n",
      "use pyfeat:\n",
      "[0.02137, 0.03387, 0.05969, 0.52555, 0.0338, 0.04099, 0.28473]\n",
      "[array([0.02137376, 0.033865  , 0.05969391, 0.5255461 , 0.03379783,\n",
      "       0.04098954, 0.28473377], dtype=float32)]\n",
      "use model directly:\n",
      "[0.02137, 0.03387, 0.05969, 0.52555, 0.0338, 0.04099, 0.28473]\n",
      "use pyfeat again:\n",
      "[0.02137, 0.03387, 0.05969, 0.52555, 0.0338, 0.04099, 0.28473]\n",
      "\n",
      "2024_11_17_18_13_06_anger_138.png\n",
      "exception occurred\n",
      "use pyfeat:\n",
      "[nan, nan, nan, nan, nan, nan, nan]\n",
      "[array([0.00630291, 0.00696982, 0.01895527, 0.3696455 , 0.02180132,\n",
      "       0.08689394, 0.4894312 ], dtype=float32)]\n",
      "use model directly:\n",
      "[0.0063, 0.00697, 0.01896, 0.36965, 0.0218, 0.08689, 0.48943]\n",
      "use pyfeat again:\n",
      "[0.0063, 0.00697, 0.01896, 0.36965, 0.0218, 0.08689, 0.48943]\n",
      "\n",
      "2024_11_17_18_10_17_anger_85.png\n",
      "use pyfeat:\n",
      "[0.01546, 0.00886, 0.02754, 0.4742, 0.01888, 0.14381, 0.31125]\n",
      "[array([0.01545792, 0.00885893, 0.02754007, 0.47420496, 0.01888004,\n",
      "       0.14380713, 0.31125087], dtype=float32)]\n",
      "use model directly:\n",
      "[0.01546, 0.00886, 0.02754, 0.4742, 0.01888, 0.14381, 0.31125]\n",
      "use pyfeat again:\n",
      "[0.01546, 0.00886, 0.02754, 0.4742, 0.01888, 0.14381, 0.31125]\n",
      "\n",
      "2024_11_17_18_09_18_anger_66.png\n",
      "use pyfeat:\n",
      "[0.00458, 0.00307, 0.01865, 0.29911, 0.0078, 0.42781, 0.23899]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_699187/3059568340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0memotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_emo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_face\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'use model directly:\\n{list(round(x, 5) for x in emotion[0].tolist())}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CameraControl/ros_dongagent_ws/src/dongagent_package/scripts/resmasknet.py\u001b[0m in \u001b[0;36mdetect_emo\u001b[0;34m(self, frame, detected_face, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m             \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mproba_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CameraControl/ros_dongagent_ws/src/dongagent_package/scripts/resmasknet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m# x = x * m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CameraControl/ros_dongagent_ws/src/dongagent_package/scripts/resmasknet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mx5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_pool5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mx5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0mx5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mx6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/CameraControl/ros_dongagent_ws/src/dongagent_package/scripts/resmasknet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pyfeat value\n",
    "from feat import Detector\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "# from SiameseRankNet import *\n",
    "from resmasknet import ResMaskNet\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "detector = Detector(emotion_model = \"resmasknet\", landmark_model='mobilefacenet')\n",
    "\n",
    "def get_face_box(start_x, start_y, end_x, end_y):\n",
    "    center_x, center_y = (start_x + end_x) // 2, (start_y + end_y) // 2\n",
    "    square_length = ((end_x - start_x) + (end_y - start_y)) // 2 // 2\n",
    "    square_length *= 1.1\n",
    "    start_x = int(center_x - square_length)\n",
    "    start_y = int(center_y - square_length)\n",
    "    end_x = int(center_x + square_length)\n",
    "    end_y = int(center_y + square_length)\n",
    "    return start_x, start_y, end_x, end_y\n",
    "\n",
    "\n",
    "result = {}\n",
    "myfolder = 'image_analysis/'\n",
    "CURR_EMO = 'anger'\n",
    "images = [x for x in os.listdir(os.path.join(myfolder, CURR_EMO)) if 'png' in x]\n",
    "image_size = (224, 224)\n",
    "transform = transforms.Compose(\n",
    "            [transforms.ToPILImage(), transforms.ToTensor()]\n",
    "        )\n",
    "model = ResMaskNet()\n",
    "for i in images[:]:\n",
    "    print(i)\n",
    "    test = os.path.join(myfolder, CURR_EMO, i)\n",
    "    res = detector.detect_image(test)\n",
    "    facebox = res.iloc[:,1:5]\n",
    "    emotions = res.iloc[:, -8:]\n",
    "    print(f'use pyfeat:\\n{[round(x, 5) for x in np.array(emotions.iloc[:, :]).tolist()[0][:-1]]}')\n",
    "    \n",
    "    frame = cv2.imread(test)\n",
    "    face = detector.detect_faces(frame)[0][:4]\n",
    "    emotion = model.detect_emo(frame=frame, detected_face=[face])\n",
    "    print(emotion)\n",
    "    print(f'use model directly:\\n{list(round(x, 5) for x in emotion[0].tolist())}')\n",
    "    \n",
    "    test = os.path.join(myfolder, CURR_EMO, i)\n",
    "    res = detector.detect_image(test)\n",
    "    facebox = res.iloc[:,1:5]\n",
    "    emotions = res.iloc[:, -8:]\n",
    "    print(f'use pyfeat again:\\n{[round(x, 5) for x in np.array(emotions.iloc[:, :]).tolist()[0][:-1]]}')\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab6b39f-c927-488d-b313-0bb7063b9b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[226.40732, 110.24718, 415.84763, 378.30368, 0.99957854]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c1ad93-37a8-4ef5-a294-cd1e3c76d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      anger   disgust      fear  happiness   sadness  surprise   neutral\n",
      "0  0.679393  0.090301  0.060653   0.024873  0.050217  0.064146  0.030418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.DataFrame(emotion, columns=[\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\"])\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd5c61-6f7a-4d6b-b83c-f093cc9ef1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ebdcc7-ba23-41fb-a9f7-86457be9c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142900/3267738695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtarget_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'disgust'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfacebox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mrmn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResMaskNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtargetID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detector' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from feat import Detector\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def get_target(emotion_name):\n",
    "    # Anger, Disgust, Fear, Happiness, Sadness, Surprise, Neutral\n",
    "    # or lowercase\n",
    "    if emotion_name in [\"Anger\", \"anger\"]:\n",
    "        return 0\n",
    "    elif emotion_name in [\"Disgust\", \"disgust\"]:\n",
    "        return 1\n",
    "    elif emotion_name in [\"Fear\", \"fear\"]:\n",
    "        return 2\n",
    "    elif emotion_name in [\"Happiness\", \"happiness\"]:\n",
    "        return 3\n",
    "    elif emotion_name in [\"Sadness\", \"sadness\"]:\n",
    "        return 4\n",
    "    elif emotion_name in [\"Surprise\", \"surprise\"]:\n",
    "        return 5\n",
    "    elif emotion_name in [\"Neutral\", \"neutral\"]:\n",
    "        return 6\n",
    "image_name = 'image_analysis/fear/2024_11_14_17_14_20_fear_0.png'\n",
    "target_emotion = 'disgust'\n",
    "\n",
    "facebox = detector.detect_faces(cv2.imread(image_name))[0]\n",
    "rmn_model = ResMaskNet()\n",
    "targetID = get_target(target_emotion)\n",
    "print(image_name)\n",
    "rmn_res = rmn_model.detect_emo(frame=cv2.imread(image_name), detected_face=[facebox])\n",
    "new_df = pd.DataFrame(rmn_res, columns=[\"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\"])\n",
    "new_df['input'] = image_name\n",
    "\n",
    "\n",
    "# -----------\n",
    "# old version\n",
    "# -----------\n",
    "image_prediction = detector.detect_image(image_name)\n",
    "df = image_prediction.head()\n",
    "\n",
    "targetID = get_target(target_emotion)\n",
    "\n",
    "assert list(new_df[target_emotion])[0] == list(df[target_emotion])[0], \"The result is not the same, old: {}, new: {}\".format(list(df[target_emotion])[0], list(new_df[target_emotion])[0])\n",
    "# return emo_df.iloc[0,targetID]\n",
    "print(new_df[target_emotion])\n",
    "print(df[target_emotion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ac933d1-46c4-472f-97ce-eabbdf1c9c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import feat\n",
    "print(feat.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d3f92c-ed84-43b7-b962-cd2da9d6197a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Detector' has no attribute '__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142900/3539268366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Detector' has no attribute '__file__'"
     ]
    }
   ],
   "source": [
    "from feat import Detector\n",
    "print(Detector.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959335b0-d0a0-47f1-8a97-b7bf57c91f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/nilearn/datasets/__init__.py:96: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Face Detection model:  retinaface\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/mobilefacenet_model_best.pth.tar\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/RF_568.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/py_feat-0.3.7-py3.7.egg/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Loading Face Landmark model:  mobilefacenet\n",
      "Loading au model:  rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator PCA from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n",
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading emotion model:  resmasknet\n"
     ]
    }
   ],
   "source": [
    "from feat import Detector\n",
    "\n",
    "detector = Detector(emotion_model = \"resmasknet\", landmark_model='mobilefacenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc8143d-788c-4e73-a748-a3fb2175fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint(detector.detect_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7092830-5274-43ef-b49f-e03ea43e6765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': (0, 255), 'x6': (0, 255), 'x8': (-255, 255), 'x10': (0, 255), 'x11': (0, 255), 'x16': (0, 255), 'x18': (-255, 255), 'x20': (0, 255), 'x28': (0, 255), 'x29': (0, 255), 'x30': (0, 255), 'x32': (0, 110)}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def generate_pbounds(axes, target_emotion):\n",
    "        pbounds_dic = {}\n",
    "        for i in axes:\n",
    "            if target_emotion == 'happiness' and i in [0, 1]:\n",
    "                # TODO need to verify\n",
    "                happy_upper = 120\n",
    "                pbounds_dic['x{}'.format(i)] = (0, happy_upper)\n",
    "            # elif i in [0, 1]:\n",
    "            #     pbounds_dic['x{}'.format(i)] = (0, 160)\n",
    "            # else:\n",
    "            pbounds_dic['x{}'.format(i)] = (0, 255)\n",
    "\n",
    "        # mouth\n",
    "        pbounds_dic['x32'] = (0, 110)\n",
    "\n",
    "        # dangerous point\n",
    "        pbounds_dic['x8'] = (-255, 255)\n",
    "        pbounds_dic['x18'] = (-255, 255)\n",
    "\n",
    "        print(pbounds_dic)\n",
    "        return pbounds_dic\n",
    "    \n",
    "target_emotion = 'target_emotion'\n",
    "all_axes_for_emotions = [1, 6, 8, 10, 11, 16, 18, 20, 28, 29, 30, 32]\n",
    "pbounds = generate_pbounds(all_axes_for_emotions, target_emotion)\n",
    "\n",
    "def middle_function(**kwargs):\n",
    "    print(kwargs)\n",
    "    return 'haha'\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "        f=middle_function,\n",
    "        # Define HyperParameter Space\n",
    "        # acquisition_function=acq,\n",
    "        pbounds = pbounds,\n",
    "        random_state=486,\n",
    "        verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ae305b-44b3-43b7-8032-5916a083ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongagent/anaconda3/envs/py37pyfeat1/lib/python3.7/site-packages/bayes_opt/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import bayes_opt\n",
    "print(bayes_opt.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "823f9dca-a068-40ec-a337-dffa9f6e5e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': 44.020503179750506, 'x10': 82.905845919157, 'x11': 69.36746392214921, 'x16': 167.22459410636617, 'x18': 173.82999527114697, 'x20': 50.111189554839456, 'x28': 84.54308039537153, 'x29': 52.69689278555022, 'x30': 45.17560626460299, 'x32': 103.14639599498683, 'x6': 27.41945974022054, 'x8': 228.30855114227967}\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "[INFO]Eye closing Constraints, search another point\n",
      "checked\n",
      "checked\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "ucb = UtilityFunction(kind='ucb',\n",
    "                          kappa=2.576,\n",
    "                          xi=0.0,\n",
    "                          kappa_decay=1,\n",
    "                          kappa_decay_delay=0)\n",
    "suggest = optimizer.suggest(ucb)\n",
    "def check_suggestion(suggestion):\n",
    "    # x1 and x6\n",
    "    print('checked')\n",
    "    if suggestion['x1'] + suggestion['x6'] < 420:\n",
    "        # if the upper lid and lower lid are too close to each other, we should not return 0\n",
    "        print(\"[INFO]Eye closing Constraints, search another point\")\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "print(suggest)\n",
    "while not check_suggestion(suggest):\n",
    "    suggest = optimizer.suggest(ucb)\n",
    "if check_suggestion(suggest):\n",
    "    print('ok')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77caf3d8-29bc-4da4-a0ac-ff96824b90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = optimizer.probe(ucb)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064874c-60bd-469c-9e9b-1533961c33b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37pyfeat1",
   "language": "python",
   "name": "py37pyfeat1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
